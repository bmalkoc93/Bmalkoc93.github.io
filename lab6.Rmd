---
title: "Lab 6: Spatial Regression I"
subtitle: <h4 style="font-style:normal">CRD 298 - Spatial Methods in Community Research</h4>
author: <h4 style="font-style:normal">Professor Noli Brazil</h4>
date: <h4 style="font-style:normal">February 13, 2019</h4>
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: cosmo
    code_folding: show
---


<style>
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
font-style: italic;
}

h1.title {
  font-weight: bold;
}

</style>
\

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The methods we have been covering up to this point have been descriptive in nature.  In this lab, you will be moving from the descriptive to the inferential side of spatial data analysis by learning how to run spatial regression models in R.  We focus on how to model spatial dependence both as a nuisance to control for and as a process of theoretical interest.  The objectives of the guide are as follows

1. Learn how to run a linear regression model
2. Learn the process for detecting spatial autocorrelation
3. Learn how to run a spatial lag model
4. Learn how to run a spatial error model

To help us accomplish these learning objectives, we will examine the association between neighborhood characteristics and violent crime rates (per Federal Bureau of Investigation guidelines, violent crimes are murder and nonnegligent manslaughter, rape, robbery, and aggravated assault) in the City of Seattle, WA. 

This lab guide assumes working knowledge of basic concepts in linear regression modelling. If you need a brushing up on linear regression modelling concepts and terms, please see this week's handout and Chapters 3 and 4 in Gelman and Hill.

<div style="margin-bottom:25px;">
</div>
## **Load necessary packages**
\

We'll be introducing the following packages in this lab 

```{r eval = FALSE}
install.packages("broom")
install.packages("corrplot")
install.packages("car")
install.packages("olsrr")
install.packages("stargazer")
install.packages("knitr")
```

You may have already installed **knitr** in the past if you've been using the function `kable()` to create presentation ready tables in your html documents.

Load in the following packages, all of which we've covered in previous labs

```{r warning = FALSE, message = FALSE}
library(sf)
library(tidyverse)
library(sp)
library(tmap)
library(spdep)
```

<div style="margin-bottom:25px;">
</div>
## **Bring in the data**
\

We will be using the shape file seattle_census_tracts_2010.shp. This file contains violent crime rates between 2014 and 2017 by census tract.  It also contains demographic and socioeconomic data from the 2012-16 American Community Survey. The record layout for the shapefile's attribute table is located [here](https://raw.githubusercontent.com/crd230/data/master/seattle_record_layout.txt).

I zipped up the files associated with the shapefile onto Github.  Download the file, unzip it, and bring it into R using the following code.

```{r warning = FALSE, message = FALSE, eval = FALSE}
setwd("insert your pathway here")
download.file(url = "https://raw.githubusercontent.com/crd230/data/master/seattle_census_tracts_2010.zip", destfile = "seattle_census_tracts_2010.zip")
unzip(zipfile = "seattle_census_tracts_2010.zip")

sea.tracts <- st_read("seattle_census_tracts_2010.shp")
```

```{r warning = FALSE, message = FALSE, include = FALSE}
download.file(url = "https://raw.githubusercontent.com/crd230/data/master/seattle_census_tracts_2010.zip", destfile = "seattle_census_tracts_2010.zip")
unzip(zipfile = "seattle_census_tracts_2010.zip")

sea.tracts <- st_read("seattle_census_tracts_2010.shp")
```

<div style="margin-bottom:25px;">
</div>
## **Standard linear regression**
\

We're interested in examining the neighborhood characteristics associated with violent crime rates in Seattle. What explanatory variables should we include?  An important determining factor is to make sure we do not pick variables that are highly correlated.  In regression, this is known as multicollinearity, and is a problem because the partial regression coefficient for any collinear variable is highly unstable. 

One way to detect multicollinearity is to view a correlation matrix. Let's produce a correlation matrix of the candidate variables using the function `cor()`. The function does not directly take in spatial objects. We use the function `st_geometry()` in the following way to make the spatial data set not spatial.

```{r}
sea.tracts.df <- sea.tracts
st_geometry(sea.tracts.df) <- NULL
```

The way to read the above is code is that `st_geometry()` calls up the geometry of *sea.tracts.df*, which we then set to NULL. This removes the geometry and makes *sea.tracts.df* a non-spatial data frame.

```{r results = "hide"}
class(sea.tracts.df)
```

We then select the variables we want to calculate correlations for and feed the resulting pared down data frame into the `cor()` function.

```{r}
corrMat <- sea.tracts.df %>% 
            select(ppov:pfhh, vcmrt1417,popd) %>%
            cor()
corrMat
```

That's a lot of numbers.  Let's visualize this matrix using a correlation plot.  We do this using the function `corrplot()` in the the *corrplot* package, which we need to load in.

```{r message = FALSE}
library(corrplot)
```

```{r}
corrplot(corrMat, method="color")
```

We find a lot of highly correlated variables.  But, which ones do we choose? Let theory and prior empirical research guide you.  Fortunately for us, there is substantive theoretical and empirical work discussing the structural neighborhood characteristics associated with the prevalence of crime.  Let's draw from this literature (for example, see [Hipp (2010)](https://academic.oup.com/socpro/article-abstract/57/2/205/1655557) and [Sampson et al. (1997)](http://science.sciencemag.org/content/277/5328/918?casa_token=FRkzNHg3zoMAAAAA:SGgF3R83W9KdKvwJV90lo54saJb56fIEvvlqq5Jt2oBNT5cltBz0pnIenQbNM3zmYmPwo3019h5MmW0)) to select the variables to include in the explanatory model.

Criminological studies typically incorporate measures of concentrated disadvantage, residential mobility, immigrant concentration, levels of racial/ethnic heterogeneity, and aspects of the built environment. How do we measure these concepts? We covered concentrated disadvantage in Week 4 (% of households on public assistance, percent poverty, unemployment rate, percent of female-headed households, percent non-Hispanic black, and percent of residents under 18 years old). Immigrant concentration can be measured by percent Hispanic and percent foreign-born.  Following the methods used in [Morenoff et al. (2001)](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1745-9125.2001.tb00932.x), we will standardize each of the variables, sum the resulting z-scores, and then divide by the number of variables in order to construct a scale of concentrated disadvantage and immigrant concentration. This produces a composite measure that evenly weights each of the original variables. We can do this using the following code, which follows closely the code we used when creating opportunity indices in [Lab 4](https://crd230.github.io/lab4.html).

```{r}
sea.tracts.std <-sea.tracts.df %>%
  select(GEOID10, ppov, unemp, pnhblack, pund18, pwelfare, pfb, phisp) %>%
  gather(variable, value, -c(GEOID10)) %>%
  group_by(variable) %>%
  mutate(mean = mean(value), sd = sd(value), z = (value-mean)/sd) %>%
  select(-(c(value, mean, sd))) %>%
  spread(variable, z) %>%
  mutate(concd = (ppov+unemp+pnhblack+pund18+pwelfare)/3, immc = (pfb+phisp)/2) %>%
  select(GEOID10, concd, immc)

sea.tracts <- left_join(sea.tracts, sea.tracts.std, by = "GEOID10")
```

Racial/ethnic heterogeneity is measured by the Herfindahl index (see, for example, page 666 in this [article](https://journals.sagepub.com/doi/10.1177/000312240707200501)). We also include the variables *mob*  *pocc*, and *popd* in the model.  Regress *lvcmrt1417* (log average yearly violent crime rates 2014-2017) on these variables using the `lm()` function.

```{r}
fit.ols <- lm(lvcmrt1417 ~ concd + mob + pocc + immc  + popd + div, 
     data = sea.tracts)
```

The first argument in `lm()` takes on a formula object. A formula is indicated by a tilde. The dependent variable *lvcmrt1417* comes first, followed by the tilde `~`, and then the independent variables separated by `+`.  The function `lm()` fits linear models.  You can run generalized linear models, which allows you to run different types of regression models (e.g. logit) including the basic linear model you run using `lm()`, using the function `glm()`. 

We can look at a summary of results using the `summary()` function

```{r}
summary(fit.ols)
```

We can make the results look "tidy" by using the function `tidy()` in the *broom* package

```{r}
library(broom)
tidy(fit.ols)
```

It appears that population density decreases crime - Jane Jacobs' [more eyes on the street](https://www.citylab.com/equity/2013/07/new-way-understanding-eyes-street/6276/) - whereas residential mobility increases crime.  

<div style="margin-bottom:25px;">
</div>
### **Diagnostic tests**
\

R has some useful commands for running diagnostics to see if our regression model has some problems, specifically if it's breaking any of the OLS standard assumptions.  First, we can verify whether multicollinearity is still a problem by calculating [Variance Inflation Factors (VIF)](https://en.wikipedia.org/wiki/Variance_inflation_factor). The higher a variable's VIF, the more collinear it is with one or more variables in the model. Use the `vif()` function in the *car* package 

```{r message =FALSE, warning = FALSE}
library(car)
```

```{r}
vif(fit.ols)
```

A variable with a VIF generally above 10 is considered to be too high.  Looks like were good! yay.

One assumption of linear regression is that errors are normally distributed. We can visually inspect for violations of this assumption through a quantile-quantile (Q-Q) plot, which plots the quantile of the residuals against the expected quantile of the standard normal distribution.

```{r message = FALSE}
#QQ plot
qqPlot(fit.ols)
```

The above figure is a quantile-comparison plot, graphing for each observation its `fit.ols` model residual on the y axis and the corresponding quantile in the t-distribution on the x axis. The dashed lines indicate 95% confidence intervals calculated under the assumption that the errors are normally distributed. If any observations fall outside this range, this is an indication that the normality assumption has been violated.

The function `shapiro.test()` runs the [Shapiro-Wilk Test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_testhttps://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test), a formal test of normality

```{r}
shapiro.test(resid(fit.ols))
```

The function `resid()` extracts the residuals (Predicted Y minus Actual Y) from the linear regression model saved in *fit.ols*.

Want more tests of normality?There is the [Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)

```{r}
ks.test(resid(fit.ols), y  = pnorm)
```

where `y = pnorm` specifies the null distribution, in this case normal.  What does the Q-Q plot and the tests for normality suggest?

Another assumption is that errors are homeskedastic - that is the variance of residuals are constant.  You can plot the residuals to explore the presence of heteroskedasticity.  If we see the spread of the points narrow or widen from left to right, heteroskedasticity is present.

```{r}
#plot residuals
plot(resid(fit.ols))
```

There are formal statistical tests to check for heteroscedasticity. One such test is the [Breusch-Pagan](https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test) test, which is located in the **lmtest** package.

```{r message = FALSE, warning = FALSE}
library(lmtest)
bptest(fit.ols)
```

What did you find?  

<div style="margin-bottom:25px;">
</div>
## **Exploratory spatial data analysis**
\

The above sections go through the steps conducted in non spatial regression analysis. The focus of this lab, however, is to account for spatial dependence in the error term and the dependent variable. Before doing any kind of spatial modelling, you should conduct an Exploratory Spatial Data Analysis (ESDA) to gain an understanding of how your data are spatially structured. 

<div style="margin-bottom:25px;">
</div>
### **Map your data**
\

The first step in ESDA is to map your dependent variable.  Map the log average violent crime rate per 100,000 residents.  


```{r}
tm_shape(sea.tracts, unit = "mi") +
  tm_polygons(col = "lvcmrt1417", style = "quantile",palette = "Reds", 
              border.alpha = 0, title = "") +
  tm_scale_bar(breaks = c(0, 1, 2), size = 1, position = c("left", "bottom")) +
  tm_compass(type = "4star", position = c("left", "top")) + 
  tm_layout(main.title = "Log violent crime rates in Seattle
            Tracts",  main.title.size = 0.95, frame = FALSE, legend.outside = TRUE)
```


Next, you'll want to map the residuals from your regression model. To extract the residuals from *fit.ols*, use the `resid()` function.  Save it back into *sea.tracts*.

```{r}
sea.tracts <- mutate(sea.tracts, olsresid = resid(fit.ols))
```

Plot it

```{r}
tm_shape(sea.tracts, unit = "mi") +
  tm_polygons(col = "olsresid", style = "quantile",palette = "Reds", 
              border.alpha = 0, title = "") +
  tm_scale_bar(breaks = c(0, 1, 2), size = 1, position = c("left", "bottom")) +
  tm_compass(type = "4star", position = c("left", "top")) + 
  tm_layout(main.title = "Residuals from linear regression in Seattle
            Tracts",  main.title.size = 0.95, frame = FALSE, legend.outside = TRUE)
```

<div style="margin-bottom:25px;">
</div>
### **Global Moran's I**
\

There appears to be evidence of clustering from the exploratory maps. What does our buddy the Moran's I tell us?  First, we need to convert *sea.tracts* to an **sp** object. 

```{r}
sea.tracts.sp <- as(sea.tracts, "Spatial")
```

Then create a neighbor object and its associated spatial weights matrix.  Let's use the standard Queen contiguity.

```{r}
seab<-poly2nb(sea.tracts.sp, queen=T)
seaw<-nb2listw(seab, style="W", zero.policy = TRUE)
```

Examine the Moran scatterplot.

```{r}
moran.plot(sea.tracts.sp$lvcmrt1417, listw=seaw, xlab="Standardized Log Violent Crime Rate", ylab="Standardized Lagged Log Viiolent Crime Rate",
main=c("Moran Scatterplot for Log Violent Crime Rate", "in Seatte") )
```

Finally, the Global Moran's I

```{r}
moran.mc(sea.tracts.sp$lvcmrt1417, seaw, nsim=999)
```

Repeat for the residuals using the `lm.morantest()` function

```{r}
lm.morantest(fit.ols, seaw)
```

You should use other spatial weight matrices to test the robustness of your ESDA results. For example, do you get similar results when using a 3-nearest neighbor definition? What about a 2000 meter distance based matrix? How about a 2000 meter distance based matrix with inverse distance weights?

```{r echo=FALSE}
sea.coords <- coordinates(sea.tracts.sp)
Seanb_distt2 <- dnearneigh(sea.coords, d1 = 0, d2 = 2000, row.names = sea.tracts.sp$GEOID10) 
Seaw_distt2<-nb2listw(Seanb_distt2, style="W", zero.policy = TRUE)
```


<div style="margin-bottom:25px;">
</div>
## **Spatial lag model**
\

Based on the exploratory mapping, Moran scatterplot, and the Moran's I, there appears to be spatial autocorrelation in the dependent variable.  This means that if there is a spatial lag process going on and we fit an OLS model our coefficients will be biased and inefficient.  That is, the coefficient size and sign are not close to their true value and its standard errors are underestimated. This means trouble, big trouble, real big trouble.

A spatial lag model can be estimated in R using the command `lagsarlm()`, which is in the **spdep** package.  Let's go with a Queen contiguity spatial weights matrix.  One potential motivation for using Queen contiguity is that the city is physically separated by a body of water.  Plotting the Queen contiguity neighborhood connections, we find no connections across Lake Union going west to Puget Sound and east to Lake Washington because the neighborhoods are not physically connected (or they are but only via bridges).  

```{r}
sea.coords <- coordinates(sea.tracts.sp)
plot(sea.tracts.sp, border = "grey60")
plot(seaw, coords = sea.coords, add=T, col=2)
```

If we use a 2000 m distance band, we get connections, as seen in the following map.

```{r echo = FALSE}
plot(sea.tracts.sp, border = "grey60")
plot(Seaw_distt2, coords = sea.coords, add=T, col=2)
```

Some have argued that physical barriers like highways and lakes create natural neighborhood boundaries [(Kramer 2018)](https://journals.sagepub.com/doi/abs/10.1177/2399808318766067).  This is an example of how one should think very carefully about how to define neighbors in a spatial analysis.  Are those neighborhoods across Lake Union "connected"?

Let's fit the spatial lag model

```{r}
fit.lag<-lagsarlm(lvcmrt1417 ~ concd + mob + pocc + immc  + popd + div +pnhblack, 
     data = sea.tracts, listw = seaw) 
summary(fit.lag)
```

The only real difference between the code for `lm()` and `lagsarlm()` is the argument `listw`, which you use to specify the spatial weights matrix.

Let's calculate the Moran's I on the model's residuals

```{r}
moran.mc(resid(fit.lag), seaw, nsim=999)
```


<div style="margin-bottom:25px;">
</div>
## **Spatial error model**
\

The spatial error model incorporates spatial dependence in the errors. If there is a spatial error process going on and we fit an OLS model our coefficients will be unbiased but inefficient.  That is, the coefficient size and sign are asymptotically correct but its standard errors are underestimated. 

We can estimate a  spatial error model in R using the command `errorsarlm()` also in the **spdep** package.

```{r}
fit.err<-errorsarlm(lvcmrt1417 ~ concd + mob + pocc + immc  + popd + div +pnhblack, 
     data = sea.tracts, listw = seaw) 
summary(fit.err)
```

And the Moran's I of the residuals

```{r}
moran.mc(resid(fit.err), seaw, nsim=999)
```

<div style="margin-bottom:25px;">
</div>
## **Presenting your results**
\

An organized table of results is an essential component not just in academic papers, but in any professional report or presentation.  Up till this point, we've been reporting results by simply printing objects, like the following

```{r results = "hide"}
fit.ols
```

We used the function `tidy()` above to create a tibble of modelling results. We can make these results prettier by using a couple of functions for making nice tables in R. First, there is the `kable()` function from the **knitr** package.   

```{r message = FALSE, warning = FALSE}
library(knitr)
```

```{r}
kable(tidy(fit.ols))
```

There are options to control the number of digits, whether row names are included or not, column alignment, and other options that depend on the output type.

The table produced by `kable()`looks good. But what if we want to present results for more than one model, such as presenting *fit.ols*, *fit.lag*, and *fit.err* side by side? We can use the `stargazer()` function from the **stargazer** package to do this.

```{r results = 'asis', warning=FALSE, message=FALSE}
library(stargazer, quietly = TRUE)
stargazer(fit.ols, fit.lag, fit.err, type = "html",
                    title="Title: Regression Results")
```


\

There are a number of options you can tweak to make the table more presentation ready, such as adding footnotes and changing column and row labels.

Note three things: First, if you ran the `stargazer()` function above directly in your R console, you'll get output that won't make sense.  Knit the document and you'll see your pretty table. Second, you'll need to add the argument `quietly = TRUE` when you load **stargazer** into your current R session. Third, you will need to add `results = 'asis'` as an RMarkdown chunk option.


<div style="margin-bottom:25px;">
</div>
## **Which model? OLS, Spatial lag, or Spatial error?**
\

As we discussed in this week's handout, there are many fit statistics and tests to determine which model is most appropriate.  A popular set of tests proposed by Anselin (1988) (also see Anselin et al. 1996) are the Lagrange Multiplier (LM) tests. The null in these tests is the OLS model.  A test showing statistical significance rejects this null. 

In general, you fit the OLS model to your dependent variable, then submit the OLS model fit to the LM testing procedure. Then go through the series of steps I outlined in this week's handout.

To run the LM tests in R, use the `lm.LMtests()` command in the **spdep** package. Run this command and interpret the results. Which model is "best"? Why?

The LM test is not the only way to select between models. We'll go through a couple of other methods in next week's lecture and lab.


***


Website created and maintained by [Noli Brazil](https://nbrazil.faculty.ucdavis.edu/)