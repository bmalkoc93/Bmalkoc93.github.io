---
title: "Lab 4a: Mapping Open Data"
subtitle: <h4 style="font-style:normal">CRD 230 - Spatial Methods in Community Research</h4>
author: <h4 style="font-style:normal">Professor Noli Brazil</h4>
date: <h4 style="font-style:normal">January 26, 2021</h4>
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: cosmo
    code_folding: show
---



<style>
p.comment {
background-color: #bdced6;
padding: 10px;
border: 0px solid black;
margin-left: 25px;
border-radius: 5px;
font-style: normal;
}

h1.title {
  font-weight: bold;
  font-family: Arial;  
}

h2.title {
  font-family: Arial;  
}

</style>


<style type="text/css">
#TOC {
  font-size: 13px;
  font-family: Arial;
}
</style>


\


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message = FALSE)
```

In [Lab 3](https://crd230.github.io/lab3.html), you learned how to process spatial data in R, focusing primarily on reading in, wrangling and mapping *areal* or *polygon* data.  In this lab, we will cover *point* data in R. More broadly, we will cover how to access and clean data from Open Data portals. In Lab 4b, we will learn how to process Big Data using Twitter Tweets as a case example. The objectives of this guide are

1. Learn how to read in point data
2. Understand Coordinate Reference Systems
3. Learn how to reproject spatial data
5. Learn how to bring in data from OpenStreetMap
6. Learn how to map point data

To achieve these objectives, we will first examine the spatial distribution of homeless encampments in the City of Los Angeles using 311 data downloaded from the city's [open data portal](https://data.lacity.org/).  This lab guide follows closely and supplements the material presented in Chapters 2.4, 4.2, 6 and 8 in the textbook [Geocomputation with R](https://geocompr.robinlovelace.net/) (GWR) and class Handout 3.  

<div style="margin-bottom:25px;">
</div>
## **Installing and loading packages**
\

You'll need to install the following packages in R.  You only need to do this once, so if you've already installed these packages, skip the code.  Also, don't put these `install.packages()` in your R Markdown document.  Copy and paste the code in the R Console.

```{r warning = FALSE, message = FALSE, eval = FALSE}
install.packages("tidygeocoder")
install.packages("osmdata")
```

You'll need to load the following packages.  Unlike installing, you will *always* need to load packages whenever you start a new R session. You'll also always need to use `library()` in your R Markdown file.

```{r warning = FALSE, message = FALSE}
library(sf)
library(tidyverse)
library(units)
library(tmap)
library(tidycensus)
library(tigris)
library(rmapshaper)
library(tidygeocoder)
library(leaflet)
library(osmdata)
```


<div style="margin-bottom:25px;">
</div>
## **Read in census tract data**
\

We will need to bring in census tract polygon features and racial composition data from the 2015-2019 American Community Survey using the Census API and keep tracts within Los Angeles city boundaries using a clip.  The code for accomplishing these tasks is below.  We won't go through each line of code in detail because we've covered all of these operations and functions in prior labs.  I've embedded comments within the code that briefly explain what each chunk is doing, but go back to prior guides (or RDS/GWR) if you need further help.  The only new code I bring in is the use of `output = "wide"` in the  `get_acs()` command.  I use the code `rename_with(~ sub("E$", "", .x), everything())` to eliminate the *E* at the end of the variable names for the estimates.  Prior to this lab, we brought in a long dataset and used `spread()` to go from long to wide.

```{r warning=FALSE, results = "hide", message=FALSE}
# Bring in census tract data. 
ca.tracts <- get_acs(geography = "tract", 
              year = 2017,
              variables = c(tpop = "B01003_001", tpopr = "B03002_001", 
                            nhwhite = "B03002_003", nhblk = "B03002_004",
                             nhasn = "B03002_006", hisp = "B03002_012"),
              state = "CA",
              survey = "acs5",
              output = "wide",
              geometry = TRUE)
# Make the data tidy, calculate percent race/ethnicity, and keep essential vars.
ca.tracts <- ca.tracts %>% 
  rename_with(~ sub("E$", "", .x), everything()) %>%
  mutate(pnhwhite = nhwhite/tpopr, pnhasn = nhasn/tpopr, 
              pnhblk = nhblk/tpopr, phisp = hisp/tpopr) %>%
  dplyr::select(c(GEOID,tpop, pnhwhite, pnhasn, pnhblk, phisp))  

# Bring in city boundary data
pl <- places(state = "CA", cb = TRUE)

# Keep LA city
la.city <- filter(pl, NAME == "Los Angeles")

#Clip tracts using LA boundary
la.city.tracts <- ms_clip(target = ca.tracts, clip = la.city, remove_slivers = TRUE)
```


<div style="margin-bottom:25px;">
</div>
## **Read in point data**
\

Point data give us the locations of objects or events within an area. Events can be things like crimes and car accidents. Objects can be things like trees, houses, jump bikes or even people, such as the locations of where people were standing during a protest. 

Often you will receive point data in tabular (non-spatial) form. These data can be in one of two formats

1. Point longitudes and latitudes (or X and Y coordinates) 
2. Street addresses

If you have longitudes and latitudes, you have all the information you need to make the data spatial. This process involves using geographic coordinates (longitude and latitude) to place points on a map. In some cases, you won’t have coordinates but street addresses. Here, you’ll need to geocode your data, which involves converting street addresses to geographic coordinates. These tasks are intimately related to the concept of projection and reprojection, and underlying all of these concepts is the Coordinate Reference System. 


<div style="margin-bottom:25px;">
</div>
### **Longitude/Latitude**
\

Best case scenario is that you have a point data set with geographic coordinates. Geographic coordinates are in the form of a longitude and latitude, where longitude is your X coordinate and spans East/West and latitude is your Y coordinate and spans North/South.

Let’s bring in a csv data set of homeless encampments in Los Angeles City, which was downloaded from the [Los Angeles City Open Data portal](https://data.lacity.org/A-Well-Run-City/MyLA311-Service-Request-Data-2019/pvft-t768). I uploaded the data set on GitHub so you can directly read it in using `read_csv()`

```{r warning = FALSE, message = FALSE}
homeless311.df <- read_csv("https://raw.githubusercontent.com/crd230/data/master/homeless311_la_2019.csv")
```

The data represent homeless encampment locations in 2019 as reported through the City's [311 system](https://www.lacity.org/311-services).  To download the data from LA's open data portal linked above, I did the following

1. Click on *View Data*.   This will bring up the  data in an excel style worksheet.
2. You'll find that there are over one million 311 requests in 2019.  Rather than bringing all of these requests into R, let's just filter for homeless encampments.  To do this, click on *Filter*,  *Add a New Filter Condition*, select *Request Type* from the first pull down menu, then type in *Homeless Encampment* in the first text box.
3. Click on *Export* and select *CSV*.  Download the file into an appropriate folder on your hard drive.

Viewing the file and checking its class you'll find that *homeless311.df* is a regular tibble, not a spatial **sf** points object.

We will use the function `st_as_sf()` to create a point **sf** object of *homeless311.df*.  The function requires you to specify the longitude and latitude of each point using the `coords =` argument, which are conveniently stored in the variables *Longitude* and *Latitude*. 

```{r warning = FALSE, message = FALSE}
homeless311.sf <- st_as_sf(homeless311.df, coords = c("Longitude", "Latitude"))
```


<div style="margin-bottom:25px;">
</div>
### **Street Addresses**
\

Often you will get point data that won’t have longitude/X and latitude/Y coordinates but instead have street addresses. The process of going from address to X/Y coordinates is known as geocoding. 

To demonstrate geocoding, type in your street address, city and state inside the quotes below.  

```{r eval = FALSE}
myaddress.df  <- tibble(street = "", city = "", state = "")
```

This creates a tibble with your street, city and state saved in three variables. To geocode addresses to longitude and latitude, use the function `geocode()` which is a part of the **tidygeocoder** package.  Use `geocode()` as follows

```{r eval = FALSE}
myaddress.df <- geocode(myaddress.df, street = street, city = city, state = state, method = "osm")
```

Here, we specify street, city and state variables.  The argument `method = 'osm'` specifies the geocoder used to map addresses to longitude/latitude locations, in the above case `'osm'` stands for [OpenStreetMaps](https://www.openstreetmap.org/#map=4/38.01/-95.84).  Think of R going to the OpenStreetMaps website, searching for each address,  plucking the latitude and longitude of your address, and saving it in a tibble named *myaddress.df*

If you view this object, you'll find the latitude *lat* and longitude *long* attached as columns. Convert this point to an **sf** object using the function `st_as_sf()`.

```{r eval = FALSE}
myaddress.sf <- st_as_sf(myaddress.df, coords = c("long", "lat"))
```

Type in `tmap_mode("view")` and then [map](https://crd230.github.io/lab3.html#Mapping_in_R) *myaddress.sf*.  Zoom into the point.  Did it get your home address correct?

<br>

Let's bring in a csv file containing the street addresses of [homeless shelters and services in Los Angeles County](https://data-lahub.opendata.arcgis.com/datasets/b0f7b2ebce0146069c74abf4b25a6688_158) , which I also downloaded from Los Angeles' open data portal.

```{r warning = FALSE, message = FALSE}
shelters.df <- read_csv("https://raw.githubusercontent.com/crd230/data/master/Homeless_Shelters_and_Services.csv")

glimpse(shelters.df)
```

The file contains no latitude and longitude data, so we need to convert the street addresses contained in the variables *addrln1*, *city* and *state*.  Use the function `geocode()`.  The process will take a few minutes so be patient.

```{r}
shelters.geo <- geocode(shelters.df, street = addrln1, city = city, state = state, method = 'osm')
```

Look at the column names.

```{r}
names(shelters.geo)
```

We see the latitudes and longitudes are attached to the variables *lat* and *long*, respectively.  Notice that not all the addresses were successfully geocoded.

```{r}
summary(shelters.geo$lat)
```

Eight shelters received an `NA`.  This is likely because the addresses are not correct, has errors, or are not fully specified.  For example, the address *11046 Vly Mall* should be written out as *11046 Valley Mall*.  You'll have to manually fix these issues, which becomes time consuming if you have a really large data set.  For the purposes of this lab, let's just discard these, but in practice, make sure to double check your address data (See the document Geocoding_Best_Practices.pdf in the Other Resources folder on Canvas for best practices for cleaning address data).

```{r}
shelters.geo <- shelters.geo %>%
                filter(is.na(lat) == FALSE & is.na(long) == FALSE)
```

Convert latitude and longitude data into spatial points using the function `st_as_sf()`.

```{r warning = FALSE, message = FALSE}
shelters.sf <- st_as_sf(shelters.geo, coords = c("long", "lat"))
```

<div style="margin-bottom:25px;">
</div>  
## **Coordinate Reference System**
\

Plot homeless encampments and shelters using functions from the **tmap** package, which we learned about in [Lab 3](https://crd230.github.io/lab2.html#tmap). This is an example of a basic pin or dot map. 

```{r warning = TRUE}
tmap_mode("plot")

tm_shape(homeless311.sf) +  
  tm_dots(col="red") +
tm_shape(shelters.sf) +  
  tm_dots(col="blue")
```

We get a map that looks correct.  But, we did get two warnings.  These warnings are not something to sneeze at -  they tell us that we haven't set a projection, which is no problem if we're just mapping, but is no good if we want to do some spatial analyses on the point locations.  

What we need to do is set the Coordinate Reference System (CRS). The CRS is an important concept to understand when dealing with spatial data.  We won't go through the *real* nuts and bolts of CRS, which you can read in GWR Chapters 2.4 and 6, but we'll go through enough of it so that you can get through most of the CRS related spatial data wrangling tasks in this class. In addition to GWR, Esri also has a nice explanation [here](https://www.esri.com/arcgis-blog/products/arcgis-pro/mapping/gcs_vs_pcs/).  This [site](https://mgimond.github.io/Spatial/coordinate-systems-in-r.html) also does a thorough job of explaining how to work with CRS in R. You can also read the document Coordinate_Reference_Systems.pdf on Canvas in the Other Resources folder.

The CRS contains two major components: the Geographic Coordinate System (GCS) and the Projected Coordinate System (PCS).  A GCS uses a three-dimensional spherical surface to define locations on the earth. The GCS is composed of two parts: the ellipse and the datum.  The ellipse is a model of the Earth's shape - how the earth’s roundness is calculated.  The datum defines the coordinate system of this model - the origin point and the axes.  You need these two basic components to place points on Earth's three-dimensional surface.  Think of it as trying to create a globe (ellipse) and figuring out where to place points on that globe (datum).

The PCS then translates these points from a globe onto a two-dimensional space.  We need to do this because were creating flat-paper or on-the-screen maps, not globes (it's kind of hard carrying a globe around when you're finding your way around a city).  

You can find out the CRS of a spatial data set using the function `st_crs()`.

```{r}
st_crs(homeless311.sf)
```

<center>
![](/Users/noli/Documents/UCD/teaching/CRD 230/Lab/crd230.github.io/1ptdm0.jpg)

</center>
\


When we used `st_as_sf()` above to create *homeless311*, we did not specify a CRS.  We should have.  Working with spatial data requires both a Geographic Coordinate System (so you know where your points are on Earth) and a Projection (a way of putting points in 2 dimensions). Both. Always. Like Peanut Butter and Jelly. Like Sonny and Cher. 

There are two common ways of specifying a coordinate system in R: via the EPSG numeric [code](http://spatialreference.org/ref/epsg/) or via the [PROJ4](https://proj4.org/apps/proj.html) formatted string. The PROJ4 syntax consists of a list of parameters, each separated by a space and prefixed with the `+` character.  To specify the PCS, you use the argument `+proj=`.  To specify the GCS, you use the arguments `+ellps=` to establish the ellipse and `+datum=` to specify the datum.  

How do we know which CRS to use? The most common datums in North America are NAD27, NAD83 and WGS84, which has the ellipsoids clrk66, GRS80, and WGS84, respectively. The datum always specifies the ellipsoid that is used, but the  ellipsoid does not specify the datum. This means you can specify `+datum=` and not specify `+ellps=` and R will know what to do, but not always the other way around.  For example, the ellipsoid GRS80 is also associated with the datum GRS87, so if you specify `ellps=GRS80` without the datum, R won't spit out an error, but will give you an unknown CRS. The most common datum and ellipsoid combinations are listed in Figure 1 in the Coordinate_Reference_Systems.pdf document on Canvas.

When you are bringing in point data with latitude and longitude, the projected coordinate system is already set for you. Latitudes and longitudes are X-Y coordinates, which is essentially a Plate Carree projection.  You specify a PCS using the argument (in quotes) `+proj=longlat`. Let's use `st_as_sf()` again on *homeless311.df*, but this time specify the CRS using the argument `crs`.

```{r}
homeless311.sf <- st_as_sf(homeless311.df, coords = c("Longitude", "Latitude"), crs = "+proj=longlat +datum=WGS84 +ellps=WGS84")
```

The CRS should have spaces only in between `+proj=longlat`, `+datum=WGS84` and `+ellps=WGS84`, and no other place.  Remember, you could have just specified `+datum=WGS84` and R would have known what to do with the ellipsoid. What is the CRS now?

```{r eval = FALSE}
st_crs(homeless311.sf)
```

We can see the PROJ4 string using

```{r}
st_crs(homeless311.sf)$proj4string
```
Instead of a PROJ4, we can specify the CRS using the EPSG associated with a GCS and PCS combination. A EPSG is a four-five digit unique number representing a particular CRS definition. The EPSG for the particular GCS and PCS combination used to create *homeless311.sf* is 4326.  Had we looked this up [here](http://spatialreference.org/ref/epsg/4326/), we could have used `crs = 4326` instead of `"+proj=longlat +datum=WGS84"` in  `st_as_sf()` as we do below.

```{r}
homeless311.sf2 <- st_as_sf(homeless311.df, coords = c("Longitude", "Latitude"), crs = 4326)
```

we verify that the CRS are the same

```{r}
st_crs(homeless311.sf) == st_crs(homeless311.sf2)
```

Let's set the CRS for the homeless shelter and services points

```{r}
shelters.sf <- st_as_sf(shelters.geo, coords = c("long", "lat"), crs = 4326)
```

Another important problem that you may encounter is that a shapefile or any spatial data set you downloaded from a source contains no CRS (unprojected or unknown).  In this case, use the function `st_set_crs()` to set the CRS.  See GWR 6.1 for more details.

<div style="margin-bottom:25px;">
</div>
### **Reprojection**
\

The above section deals with a situation where you are establishing the CRS for the first time.  However, you may want to change an already defined CRS.  This task is known as reprojection. Why would you want to do this? There are three main reasons: 

1. Two spatial objects that are compared or combined have a different CRS.
2. Many geometric functions require a certain CRS.
3. Aesthetic purposes and/or to correct distortions.

Reason 1: All spatial data in your current R session should have the same CRS if you want to overlay the objects on a map or conduct any of the multiple layer spatial operations we went through in [Lab 3](https://crd230.github.io/lab3.html#Spatial_Data_Wrangling).

Let's check to see if *homeless.sf* and *shelters.sf* have the same CRS 
 
```{r}
st_crs(homeless311.sf) == st_crs(shelters.sf)
```

Great. Do they match with *la.city.tracts*

```{r}
st_crs(homeless311.sf) == st_crs(la.city.tracts)
```

Oh no! If you map *homeless311.sf* and *la.city.tracts*, you'll find that they align.  But R is smart enough to reproject on the fly to get them on the same map. However, this does not always happen.  Furthermore, R doesn't actually change the CRS. This leads to the next reason why we may  need to reproject.

 Many of R's geometric functions that require calculating distances (e.g. distance from one point to another) or areas require a standard measure of distance/area. The spatial point data of homeless encampments are in longitude/latitude coordinates.  Distance in longitude/latitude is in decimal degrees, which is not a standard measure.  We can find out the units of a spatial data set by using the `st_crs()` function and calling up units as follows

```{r}
st_crs(homeless311.sf)$units
st_crs(la.city.tracts)$units
```


Not good. Not only do we need to reproject *homeless311.sf*, *shelters.sf*, and *la.city.tracts* into the same CRS, we need to reproject them to a CRS that handles standard distance measures such as meters or kilometers.  The [Universal Transverse Mercator](https://desktop.arcgis.com/en/arcmap/10.3/guide-books/map-projections/universal-transverse-mercator.htm) (UTM) projected coordinate system works in meters.  UTM separates the United States in separate zones and Southern California is in zone 11, as shown in the figure below.  


<center>
![Figure 2: UTM Zones](/Users/noli/Documents/UCD/teaching/CRD 230/Lab/crd230.github.io/utm.png)

</center>

Let's reproject *la.city.tracts*,  *homeless311.sf* and *shelters.sf* to a UTM Zone 11N projected coordinate system. Use `+proj=utm ` as the PCS, NAD83 as the datum and GRS80 as the ellipse (popular choices for the projection/datum/ellipse of the U.S.).  Whenever you use UTM, you also need to specify the zone, which we do by using `+zone=11N`. To reproject use the function `st_transform()` as follows.

```{r}
la.city.tracts.utm <-st_transform(la.city.tracts, 
                                 crs = "+proj=utm +zone=11N +datum=NAD83 +ellps=GRS80") 

homeless.sf.utm <- st_transform(homeless311.sf, 
                                 crs = "+proj=utm +zone=11N +datum=NAD83 +ellps=GRS80") 
shelters.sf.utm <- st_transform(shelters.sf, 
                                 crs = "+proj=utm +zone=11N +datum=NAD83 +ellps=GRS80")
```

Equal?

```{r}
st_crs(la.city.tracts.utm) == st_crs(homeless.sf.utm)
```

Units?

```{r}
st_crs(la.city.tracts.utm)$units
st_crs(homeless.sf.utm)$units
st_crs(shelters.sf.utm)$units
```

"m" stands for meters. 

Note that you cannot change the CRS if one has not already been established. For example, you cannot use the function `st_transform()` on *homeless311.sf* if you did not establish the CRS when you used `st_as_sf()` on *homeless311.df*.

Now, let's map em all.

```{r}
tm_shape(la.city.tracts) +
  tm_polygons() +
tm_shape(homeless.sf.utm) +  
  tm_dots(col="red") +
tm_shape(shelters.sf.utm) +  
  tm_dots(col="blue")
```

Main takeaway points:

1. The CRS for any spatial data set you create or bring into R should always be established.
2. If you are planning to work with multiple spatial data sets in the same project, make sure they have the same CRS.
3. Make sure the CRS is appropriate for the types of spatial analyses you are planning to conduct. 

If you stick with these principles, you should be able to get through most issues regarding CRSs. If you get stuck, read GWR Ch. 2.4 and 6.

<div style="margin-bottom:25px;">
</div>
## **OpenStreetMap**
\

Another way to bring point data into R is to draw from the wealth of spatial data offered by OpenStreetMap (OSM). OSM is  a free and open map of the world created largely by the voluntary contributions of millions of people around the world. Since the data are free and open, there are few restrictions to obtaining and using the data. The only condition of using OSM data is proper attribution to OSM contributors.

We can grab a lot of really cool data from OSM using their API. OSM serves two APIs, namely Main API for editing OSM, and Overpass API for providing OSM data. We will use Overpass API to gather data in this lab.  What kinds of things can you bring into R through their API? A lot. Check them out on their [Wiki](https://wiki.openstreetmap.org/wiki/Map_Features).

Data can be queried for download using a combination of search criteria like location and type of objects. It helps to understand how OSM data are structured. OSM data are stored as a list of attributes tagged in key - value pairs of geospatial objects (points, lines or polygons). 

Maybe were interested in the proximity of homeless encampments to restaurants. We can bring in restaurants listed by OSM using various functions from the package **osmdata**. Restaurants are tagged under amenities. Amenities are facilities used by visitors and residents. Here, ‘key’ is “amenity” and ‘value’ is “restaurant.” Other amenities include: “university”, “music school”, and “kindergarten” in education, “bus station”, “fuel”, “parking” and others in transportation, and much more.

Use the following line of code to get restaurants in Los Angeles

```{r}
data_from_osm_df <- opq(getbb ("Los Angeles, California")) %>% #gets bounding box
  add_osm_feature(key = "amenity", value = "restaurant") %>% #searches for restaurants within the bounding box
  osmdata_sf() #download OSM data as sf
```

What you get is a list with a lot of information about restaurants mapped in OSM.  Let's extract the geometry and name of the restaurant.  

```{r}
#select name and geometry from point data for restaurants
resto_osm <- data_from_osm_df$osm_points %>% #select point data from downloaded OSM data
  select(name, geometry) #selecting the name and geometry to plot
```

We get an **sf** object containing restaurants in Los Angeles.  

Finally, we can plot the restaurants using our comrade `leaflet()`, which we discovered in [Lab 3](https://crd230.github.io/lab3.html#leaflet).

```{r}
#create a plot in leaflet
leaflet() %>%
  addProviderTiles("CartoDB.Positron") %>%
  addCircles(data = resto_osm)
```

  
<div style="margin-bottom:25px;">
</div>
## **Mapping point patterns**
\

Other than mapping their locations (e.g. dot map), what else can we do with point locations? One of the simplest analyses we can do with point data is to examine the distribution of points across an area (also known as point density). When working with neighborhoods, we can examine point distributions by summing up the number of points in each neighborhood. To get the count of homeless encampments by census tract, we can utilize the **tidyverse** and **sf** functions we learned in the last two labs. Note that most of these functions are not new, so I won't go into intricate detail on what each line of code is doing. We first use `st_join()` to perform a spatial join with the encampments and tracts using `st_intersects()`, which we learned about in [Lab 3](https://crd230.github.io/lab3.html#Intersect):
 
```{r}
la.city.tracts.homeless <- homeless.sf.utm  %>% 
  st_join(la.city.tracts.utm, join = st_intersects) 

glimpse(la.city.tracts.homeless)
```

Now we group by a variable using `group_by()` that uniquely identifies the census tracts, (we choose *GEOID*) and use `summarize()` to create a variable *hcamps* that counts the points for each tract using the function `n()`

```{r}
la.city.tracts.homeless <- la.city.tracts.homeless %>%
        group_by(GEOID) %>% 
        summarize(hcamps = n()) 
```

Note that tracts that do not have any homeless encampments will not appear in *la.city.tracts.homeless*.  We'll join *la.city.tracts.homeless* back to the original tract file and assign a 0 to tracts with no encampments using the `replace_na()` function. We have to first drop *la.city.tracts.homeless*'s geometry because you cannot join two **sf** objects together using `left_join()`.  You drop the geometry using the `st_drop_geometry()` function.

```{r}
la.city.tracts.homeless <- st_drop_geometry(la.city.tracts.homeless)
```

Then perform the left join and replace NAs with 0.

```{r}
la.city.tracts.utm <- la.city.tracts.utm %>%
                      left_join(la.city.tracts.homeless, by = "GEOID") %>%
                      mutate(hcamps = replace_na(hcamps, 0))
```

We can map the count of encampments by census tract, but counts do not take into consideration exposure (remember the discussion regarding counts vs rates in Handout 2).  In this case, tracts that are larger in size will likely have more encampments. Let's calculate the number of encampments per area.

To calculate the number of encampments per area, we'll need to get the area of each polygon, which we do by using the function `st_area()`.  The default area metric is kilometers squared, but we can use the function `set_units()` from the **units** package to set the unit of measurement to (the U.S. friendly) miles squared `value = mi2`. Use these functions within `mutate()` to create a new column *area* that contains each tract's area.

```{r warning=FALSE, message=FALSE}
la.city.tracts.utm<- la.city.tracts.utm %>%      
                  mutate(area=set_units(st_area(la.city.tracts.utm), value = mi2))
```

The class of variable *area* is *units*

```{r}
class(la.city.tracts.utm$area)
```

We don't want it in class units, but as class numeric. Convert it to numeric using `as.numeric()`

```{r}
la.city.tracts.utm <- la.city.tracts.utm %>%
                      mutate(area = as.numeric(area))
```

Then calculate the number of homeless encampments per area.

```{r warning=FALSE, message=FALSE}
la.city.tracts.utm<-mutate(la.city.tracts.utm,harea=hcamps/area)
```

Let's create a choropleth map of encampments per area.

```{r}
tm_shape(la.city.tracts.utm, unit = "mi") +
  tm_polygons(col = "harea", style = "quantile",palette = "Reds", 
              border.alpha = 0, title = expression("Encampments per " * mi^2)) +
  tm_scale_bar(position = c("left", "bottom")) +
    tm_layout(main.title = "Homeless encampments in Los Angeles Tracts 2019",
            main.title.size = 0.95, frame = FALSE,
            legend.outside = TRUE, legend.outside.position = "right")
```                    

What is the correlation between neighborhood encampments per area and percent black? What about percent Hispanic? Use the function `cor()`.

```{r}
cor(la.city.tracts.utm$harea, la.city.tracts.utm$pnhblk, use = "complete.obs")
cor(la.city.tracts.utm$harea, la.city.tracts.utm$phisp, use = "complete.obs")
```

<p class="comment">**Practice Exercise**: Instead of encampments per area, map encampments per population *tpop*.  What is the correlation between neighborhood encampments per population and percent black? What about percent Hispanic?</p> 

<br>
<div style="margin-bottom:25px;">
</div>
## **Kernel density map**
\


In Handout 2, I briefly describe the use of kernel density maps to show the spatial patterns of points. These are also commonly known as heat maps. They are cool looking and as long as you understand broadly how these maps are created and what they are showing, they are a good exploratory tool. Also, a benefit of using a kernel density map to visually present your point data is that it does away with predefined areas like census tracts.  Your point space becomes continuous.

To create a heat map, we turn to our new friend `ggplot()`, which we met in [Lab 2](https://crd230.github.io/lab2.html#Summarizing_variables_using_graphs).  We did not cover this in last lab, but you can actually use `ggplot()` (as opposed to **tmap** or **leaflet**) to create maps (if you would like a tutorial for mapping with **ggplot**, check [this site](https://www.r-spatial.org/r/2018/10/25/ggplot2-sf.html)). Remember that `ggplot()` is built on a series of `<GEOM_FUNCTION>()`, which is a unique function indicating the type of graph you want to plot.  In the case of a kernel density map, `stat_density2d()` is the `<GEOM_FUNCTION>()`.  `stat_density2d()`  uses the `kde2d()` function in the base **MASS** package on the backend to estimate the density using a bivariate normal kernel.  

Let's create a heat map with `stat_density2d()` where areas with darker red colors have a higher density of encampments. 

```{r}
ggplot() + 
  stat_density2d(data = homeless311.df, aes(x = Longitude, y = Latitude, 
                                            fill = ..level.., alpha = ..level..),
                 alpha = .5, bins = 50, geom = "polygon") +
  geom_sf(data=la.city, fill=NA, color='black') +
  scale_fill_gradient(low = "blue", high = "red") + 
  ggtitle("Homeless Encampments Heat Map") + 
  theme_void() + theme(legend.position = "none")
```

Rather than the **sf** object *homeless311.sf*, we use the regular tibble *homeless311.df*, and indicate in `aes()` the longitude and latitude values of the homeless encampments. The argumment `bins = 50` specifies how finely grained we want to show the variation in encampments over space - the higher it is, the more granular (use a higher value than 50 to see what we mean).  We add the Los Angeles city boundary using the `geom_sf()`, which is the `<GEOM_FUNCTION>()` for mapping **sf** objects. We use `scale_fill_gradient()` to specify the color scheme where areas of low encampments density are blue and areas of high encampments density are red.



***

<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.


Website created and maintained by [Noli Brazil](https://nbrazil.faculty.ucdavis.edu/)

